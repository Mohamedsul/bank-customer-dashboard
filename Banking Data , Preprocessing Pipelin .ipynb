{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1ad72e",
   "metadata": {},
   "source": [
    "# Step 1: Banking Data Hackathon - Initial Data Assessment\n",
    "## Mohamed Alnor - Data Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3784de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BANKING DATA HACKATHON \n",
      "Analyst: Mohamed Alnor\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "print (\"=\" * 60)\n",
    "print(\"BANKING DATA HACKATHON \")\n",
    "print(\"Analyst: Mohamed Alnor\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcf0d38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the  all the datasets \n",
    "cards_df = pd.read_csv('cards_data.csv')\n",
    "transactions_df = pd.read_csv('transactions_data.csv') \n",
    "users_df = pd.read_csv('users_data.csv')\n",
    "\n",
    "print(\"All datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35fd517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cards Data (first 5 rows):\n",
      "     id  client_id  card_brand        card_type       card_number  expires  \\\n",
      "0  4524        825        Visa            Debit  4344676511950444  12/2022   \n",
      "1  2731        825        Visa            Debit  4956965974959986  12/2020   \n",
      "2  3701        825        Visa            Debit  4582313478255491  02/2024   \n",
      "3    42        825        Visa           Credit  4879494103069057  08/2024   \n",
      "4  4659        825  Mastercard  Debit (Prepaid)  5722874738736011  03/2009   \n",
      "\n",
      "   cvv has_chip  num_cards_issued credit_limit acct_open_date  \\\n",
      "0  623      YES                 2       $24295        09/2002   \n",
      "1  393      YES                 2       $21968        04/2014   \n",
      "2  719      YES                 2       $46414        07/2003   \n",
      "3  693       NO                 1       $12400        01/2003   \n",
      "4   75      YES                 1          $28        09/2008   \n",
      "\n",
      "   year_pin_last_changed card_on_dark_web  \n",
      "0                   2008               No  \n",
      "1                   2014               No  \n",
      "2                   2004               No  \n",
      "3                   2012               No  \n",
      "4                   2009               No   \n",
      "\n",
      "Transactions Data (first 5 rows):\n",
      "        id                 date  client_id  card_id   amount  \\\n",
      "0  7475327  2010-01-01 00:01:00       1556     2972  $-77.00   \n",
      "1  7475328  2010-01-01 00:02:00        561     4575   $14.57   \n",
      "2  7475329  2010-01-01 00:02:00       1129      102   $80.00   \n",
      "3  7475331  2010-01-01 00:05:00        430     2860  $200.00   \n",
      "4  7475332  2010-01-01 00:06:00        848     3915   $46.41   \n",
      "\n",
      "            use_chip  merchant_id merchant_city merchant_state      zip   mcc  \\\n",
      "0  Swipe Transaction        59935        Beulah             ND  58523.0  5499   \n",
      "1  Swipe Transaction        67570    Bettendorf             IA  52722.0  5311   \n",
      "2  Swipe Transaction        27092         Vista             CA  92084.0  4829   \n",
      "3  Swipe Transaction        27092   Crown Point             IN  46307.0  4829   \n",
      "4  Swipe Transaction        13051       Harwood             MD  20776.0  5813   \n",
      "\n",
      "  errors  \n",
      "0    NaN  \n",
      "1    NaN  \n",
      "2    NaN  \n",
      "3    NaN  \n",
      "4    NaN   \n",
      "\n",
      "Users Data (first 5 rows):\n",
      "     id  current_age  retirement_age  birth_year  birth_month  gender  \\\n",
      "0   825           53              66        1966           11  Female   \n",
      "1  1746           53              68        1966           12  Female   \n",
      "2  1718           81              67        1938           11  Female   \n",
      "3   708           63              63        1957            1  Female   \n",
      "4  1164           43              70        1976            9    Male   \n",
      "\n",
      "                    address  latitude  longitude per_capita_income  \\\n",
      "0             462 Rose Lane     34.15    -117.76            $29278   \n",
      "1    3606 Federal Boulevard     40.76     -73.74            $37891   \n",
      "2           766 Third Drive     34.02    -117.89            $22681   \n",
      "3          3 Madison Street     40.71     -73.99           $163145   \n",
      "4  9620 Valley Stream Drive     37.76    -122.44            $53797   \n",
      "\n",
      "  yearly_income total_debt  credit_score  num_credit_cards  \n",
      "0        $59696    $127613           787                 5  \n",
      "1        $77254    $191349           701                 5  \n",
      "2        $33483       $196           698                 5  \n",
      "3       $249925    $202328           722                 4  \n",
      "4       $109687    $183855           675                 1   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking at the first rows of each dataset\n",
    "print(\"Cards Data (first 5 rows):\")\n",
    "print(cards_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Transactions Data (first 5 rows):\")\n",
    "print(transactions_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Users Data (first 5 rows):\")\n",
    "print(users_df.head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88b6d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cards Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6146 entries, 0 to 6145\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   id                     6146 non-null   int64 \n",
      " 1   client_id              6146 non-null   int64 \n",
      " 2   card_brand             6146 non-null   object\n",
      " 3   card_type              6146 non-null   object\n",
      " 4   card_number            6146 non-null   int64 \n",
      " 5   expires                6146 non-null   object\n",
      " 6   cvv                    6146 non-null   int64 \n",
      " 7   has_chip               6146 non-null   object\n",
      " 8   num_cards_issued       6146 non-null   int64 \n",
      " 9   credit_limit           6146 non-null   object\n",
      " 10  acct_open_date         6146 non-null   object\n",
      " 11  year_pin_last_changed  6146 non-null   int64 \n",
      " 12  card_on_dark_web       6146 non-null   object\n",
      "dtypes: int64(6), object(7)\n",
      "memory usage: 624.3+ KB\n",
      "\n",
      "\n",
      "Transactions Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13305915 entries, 0 to 13305914\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   id              int64  \n",
      " 1   date            object \n",
      " 2   client_id       int64  \n",
      " 3   card_id         int64  \n",
      " 4   amount          object \n",
      " 5   use_chip        object \n",
      " 6   merchant_id     int64  \n",
      " 7   merchant_city   object \n",
      " 8   merchant_state  object \n",
      " 9   zip             float64\n",
      " 10  mcc             int64  \n",
      " 11  errors          object \n",
      "dtypes: float64(1), int64(5), object(6)\n",
      "memory usage: 1.2+ GB\n",
      "\n",
      "\n",
      "Users Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 2000 non-null   int64  \n",
      " 1   current_age        2000 non-null   int64  \n",
      " 2   retirement_age     2000 non-null   int64  \n",
      " 3   birth_year         2000 non-null   int64  \n",
      " 4   birth_month        2000 non-null   int64  \n",
      " 5   gender             2000 non-null   object \n",
      " 6   address            2000 non-null   object \n",
      " 7   latitude           2000 non-null   float64\n",
      " 8   longitude          2000 non-null   float64\n",
      " 9   per_capita_income  2000 non-null   object \n",
      " 10  yearly_income      2000 non-null   object \n",
      " 11  total_debt         2000 non-null   object \n",
      " 12  credit_score       2000 non-null   int64  \n",
      " 13  num_credit_cards   2000 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(5)\n",
      "memory usage: 218.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check the columns information for each dataset\n",
    "print(\"Cards Data Info:\")\n",
    "cards_df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Transactions Data Info:\")\n",
    "transactions_df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Users Data Info:\")\n",
    "users_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d483ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning & Preprocessing\n",
    " \n",
    "def clean_monetary_columns(df, columns):\n",
    "    \"\"\" Clean monetary columns by removing $ signs and converting to float\"\"\"\n",
    "    for col in columns :\n",
    "        if col in df.columns:\n",
    "            # Remove $ sign and convert to numeric\n",
    "            df[col] = df[col].str.replace('$', '').str.replace(',','')\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9e425fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cards_data(cards_df):\n",
    "    \"\"\"Clean and preprocess cards data\"\"\"\n",
    "    print(\"Processing Cards Data...\")\n",
    "    \n",
    "    # Clean monetary columns\n",
    "    cards_clean = clean_monetary_columns(cards_df.copy(), ['credit_limit'])\n",
    "    \n",
    "    # Convert dates\n",
    "    cards_clean['expires'] = pd.to_datetime(cards_clean['expires'], format='%m/%Y', errors='coerce')\n",
    "    cards_clean['acct_open_date'] = pd.to_datetime(cards_clean['acct_open_date'], format='%m/%Y', errors='coerce')\n",
    "    \n",
    "    # Create derived features\n",
    "    cards_clean['card_age_years'] = (datetime.now() - cards_clean['acct_open_date']).dt.days / 365.25\n",
    "    cards_clean['is_expired'] = cards_clean['expires'] < datetime.now()\n",
    "    cards_clean['pin_change_recency'] = datetime.now().year - cards_clean['year_pin_last_changed']\n",
    "\n",
    "    # Binary encoding for categorical variables\n",
    "    cards_clean['has_chip_binary'] = (cards_clean['has_chip'] == 'YES').astype(int)\n",
    "    cards_clean['dark_web_binary'] = (cards_clean['card_on_dark_web'] == 'Yes').astype(int)\n",
    "    \n",
    "    print(f\"Cards Data: {len(cards_clean)} records processed\")\n",
    "    return cards_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d9ef59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_users_data(users_df):\n",
    "    \"\"\"Clean and preprocess users data\"\"\"\n",
    "    print(\"Processing Users Data...\")\n",
    "    \n",
    "    # Clean monetary columns\n",
    "    users_clean = clean_monetary_columns(users_df.copy(), \n",
    "                                       ['per_capita_income', 'yearly_income', 'total_debt'])\n",
    "    \n",
    "    # Create derived financial metrics\n",
    "    users_clean['debt_to_income_ratio'] = users_clean['total_debt'] / users_clean['yearly_income']\n",
    "    users_clean['debt_to_income_ratio'] = users_clean['debt_to_income_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Age-based segments\n",
    "    users_clean['age_segment'] = pd.cut(users_clean['current_age'], \n",
    "                                      bins=[0, 25, 35, 50, 65, 100], \n",
    "                                      labels=['Gen Z', 'Millennial', 'Gen X', 'Boomer', 'Silent'])\n",
    "    \n",
    "    # Income segments\n",
    "    users_clean['income_segment'] = pd.cut(users_clean['yearly_income'], \n",
    "                                         bins=[0, 30000, 60000, 100000, np.inf], \n",
    "                                         labels=['Low', 'Medium', 'High', 'Premium'])\n",
    "    \n",
    "    # Risk categories based on credit score\n",
    "    users_clean['risk_category'] = pd.cut(users_clean['credit_score'], \n",
    "                                        bins=[0, 580, 670, 740, 850], \n",
    "                                        labels=['High Risk', 'Fair', 'Good', 'Excellent'])\n",
    "    \n",
    "    print(f\"Users Data: {len(users_clean)} records processed\")\n",
    "    return users_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "691b15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transactions_data(trans_df, sample_size=100000):\n",
    "    \"\"\"Clean and preprocess transactions data (with sampling for performance)\"\"\"\n",
    "    print(\"Processing Transactions Data...\")\n",
    "    print(f\" Large dataset detected ({len(trans_df):,} rows). Sampling {sample_size:,} for analysis...\")\n",
    "    \n",
    "    # Sample for performance (we can adjust this based on system capacity)\n",
    "    if len(trans_df) > sample_size:\n",
    "        trans_sample = trans_df.sample(n=sample_size, random_state=42).copy()\n",
    "    else:\n",
    "        trans_sample = trans_df.copy()\n",
    "    \n",
    "    # Clean monetary columns\n",
    "    trans_clean = clean_monetary_columns(trans_sample, ['amount'])\n",
    "    \n",
    "    # Convert date column\n",
    "    trans_clean['date'] = pd.to_datetime(trans_clean['date'], errors='coerce')\n",
    "    \n",
    "    # Create time-based features\n",
    "    trans_clean['year'] = trans_clean['date'].dt.year\n",
    "    trans_clean['month'] = trans_clean['date'].dt.month\n",
    "    trans_clean['day_of_week'] = trans_clean['date'].dt.dayofweek\n",
    "    trans_clean['hour'] = trans_clean['date'].dt.hour\n",
    "    trans_clean['is_weekend'] = trans_clean['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Transaction type analysis\n",
    "    trans_clean['transaction_type'] = np.where(trans_clean['amount'] < 0, 'Withdrawal', 'Payment')\n",
    "    trans_clean['amount_abs'] = abs(trans_clean['amount'])\n",
    "    \n",
    "    # Chip usage binary\n",
    "    trans_clean['chip_used'] = (trans_clean['use_chip'] == 'Chip Transaction').astype(int)\n",
    "    \n",
    "    # Error indicator\n",
    "    trans_clean['has_error'] = trans_clean['errors'].notna().astype(int)\n",
    "    \n",
    "    print(f\"Transactions Data: {len(trans_clean):,} records processed (sampled)\")\n",
    "    return trans_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "423d67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_master_dataset(users_clean, cards_clean, trans_clean):\n",
    "    \"\"\"Merge all datasets into a comprehensive master dataset\"\"\"\n",
    "    print(\" Creating Master Dataset...\")\n",
    "\n",
    "    # Drop client_id from cards to avoid duplicate columns\n",
    "    if 'client_id' in cards_clean.columns:\n",
    "        cards_clean = cards_clean.drop(columns=['client_id'])\n",
    "    \n",
    "    # Start with transactions and join cards\n",
    "    master = trans_clean.merge(cards_clean, left_on='card_id', right_on='id', \n",
    "                               how='left', suffixes=('_trans', '_card'))\n",
    "    \n",
    "    # Join with users (transactions.client_id -> users.id)\n",
    "    master = master.merge(users_clean, left_on='client_id', right_on='id', \n",
    "                          how='left', suffixes=('', '_user'))\n",
    "    \n",
    "    print(f\" Master Dataset: {len(master):,} records created\")\n",
    "    return master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e313d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing_pipeline(users_df, cards_df, trans_df):\n",
    "    \"\"\"Execute complete data preprocessing pipeline\"\"\"\n",
    "    print(\" Starting Banking Data Preprocessing Pipeline...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Process each dataset\n",
    "    users_clean = preprocess_users_data(users_df)\n",
    "    cards_clean = preprocess_cards_data(cards_df)\n",
    "    trans_clean = preprocess_transactions_data(trans_df)\n",
    "    \n",
    "    # Create master dataset\n",
    "    master_df = create_master_dataset(users_clean, cards_clean, trans_clean)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\" Preprocessing Complete!\")\n",
    "    print(f\" Final Dataset Shape: {master_df.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'users': users_clean,\n",
    "        'cards': cards_clean,\n",
    "        'transactions': trans_clean,\n",
    "        'master': master_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6287df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment function\n",
    "def assess_data_quality(datasets):\n",
    "    \"\"\"Assess data quality across all datasets\"\"\"\n",
    "    print(\"\\n Data Quality Assessment:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        print(f\"\\n{name.upper()} Dataset:\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        print(f\"  Missing Values: {df.isnull().sum().sum()}\")\n",
    "        print(f\"  Duplicates: {df.duplicated().sum()}\")\n",
    "        \n",
    "        if 'amount' in df.columns:\n",
    "            print(f\"  Amount Range: ${df['amount'].min():.2f} to ${df['amount'].max():.2f}\")\n",
    "        \n",
    "        if 'credit_score' in df.columns:\n",
    "            print(f\"  Credit Score Range: {df['credit_score'].min()} to {df['credit_score'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d8fd3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Banking Data Preprocessing Pipeline...\n",
      "==================================================\n",
      "Processing Users Data...\n",
      "Users Data: 2000 records processed\n",
      "Processing Cards Data...\n",
      "Cards Data: 6146 records processed\n",
      "Processing Transactions Data...\n",
      " Large dataset detected (13,305,915 rows). Sampling 100,000 for analysis...\n",
      "Transactions Data: 100,000 records processed (sampled)\n",
      " Creating Master Dataset...\n",
      " Master Dataset: 100,000 records created\n",
      "==================================================\n",
      " Preprocessing Complete!\n",
      " Final Dataset Shape: (100000, 56)\n",
      "\n",
      " Data Quality Assessment:\n",
      "========================================\n",
      "\n",
      "USERS Dataset:\n",
      "  Shape: (2000, 18)\n",
      "  Missing Values: 1\n",
      "  Duplicates: 0\n",
      "  Credit Score Range: 480 to 850\n",
      "\n",
      "CARDS Dataset:\n",
      "  Shape: (6146, 18)\n",
      "  Missing Values: 0\n",
      "  Duplicates: 0\n",
      "\n",
      "TRANSACTIONS Dataset:\n",
      "  Shape: (100000, 21)\n",
      "  Missing Values: 122624\n",
      "  Duplicates: 0\n",
      "  Amount Range: $-498.00 to $2554.70\n",
      "\n",
      "MASTER Dataset:\n",
      "  Shape: (100000, 56)\n",
      "  Missing Values: 122740\n",
      "  Duplicates: 0\n",
      "  Amount Range: $-498.00 to $2554.70\n",
      "  Credit Score Range: 488 to 850\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "datasets = run_preprocessing_pipeline(users_df, cards_df, transactions_df)\n",
    "assess_data_quality(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df6b13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(datasets):\n",
    "    \"\"\"Comprehensive missing value analysis\"\"\"\n",
    "    print(\" Missing Values Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        print(f\"\\n {name.upper()} Dataset Missing Values:\")\n",
    "        missing_summary = df.isnull().sum()\n",
    "        missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "        \n",
    "        if len(missing_summary) > 0:\n",
    "            missing_pct = (missing_summary / len(df) * 100).round(2)\n",
    "            missing_df = pd.DataFrame({\n",
    "                'Missing_Count': missing_summary,\n",
    "                'Missing_Percentage': missing_pct\n",
    "            })\n",
    "            print(missing_df)\n",
    "            \n",
    "            # Identify patterns in missing data\n",
    "            print(f\"\\n Critical Missing Columns (>10%):\")\n",
    "            critical_missing = missing_df[missing_df['Missing_Percentage'] > 10]\n",
    "            if len(critical_missing) > 0:\n",
    "                print(critical_missing)\n",
    "            else:\n",
    "                print(\" No critical missing values found\")\n",
    "        else:\n",
    "            print(\" No missing values\")\n",
    "    \n",
    "    return missing_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21d6ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(datasets):\n",
    "    \"\"\"Smart missing value treatment based on data type and business logic\"\"\"\n",
    "    print(\"\\n Handling Missing Values...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    cleaned_datasets = {}\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        df_clean = df.copy()\n",
    "        print(f\"\\n Processing {name.upper()} dataset...\")\n",
    "        \n",
    "        # Handle different types of missing values\n",
    "        for col in df_clean.columns:\n",
    "            missing_count = df_clean[col].isnull().sum()\n",
    "            \n",
    "            if missing_count > 0:\n",
    "                missing_pct = (missing_count / len(df_clean)) * 100\n",
    "                \n",
    "                # Geographic/Location data\n",
    "                if col in ['zip', 'merchant_city', 'merchant_state']:\n",
    "                    df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "                    print(f\"  ✓ {col}: Filled {missing_count} values with 'Unknown'\")\n",
    "                \n",
    "                # Error columns (likely means no error occurred)\n",
    "                elif 'error' in col.lower():\n",
    "                    df_clean[col] = df_clean[col].fillna('No Error')\n",
    "                    print(f\"  ✓ {col}: Filled {missing_count} values with 'No Error'\")\n",
    "                \n",
    "                # Numerical columns - use median for financial data\n",
    "                elif df_clean[col].dtype in ['float64', 'int64']:\n",
    "                    if 'income' in col.lower() or 'debt' in col.lower() or 'limit' in col.lower():\n",
    "                        # Use median for financial metrics (more robust to outliers)\n",
    "                        fill_value = df_clean[col].median()\n",
    "                        df_clean[col] = df_clean[col].fillna(fill_value)\n",
    "                        print(f\"  ✓ {col}: Filled {missing_count} values with median ({fill_value:.2f})\")\n",
    "                    else:\n",
    "                        # Use mean for other numerical data\n",
    "                        fill_value = df_clean[col].mean()\n",
    "                        df_clean[col] = df_clean[col].fillna(fill_value)\n",
    "                        print(f\"  ✓ {col}: Filled {missing_count} values with mean ({fill_value:.2f})\")\n",
    "                \n",
    "                # Categorical columns\n",
    "                elif df_clean[col].dtype == 'object':\n",
    "                    mode_value = df_clean[col].mode()\n",
    "                    if len(mode_value) > 0:\n",
    "                        df_clean[col] = df_clean[col].fillna(mode_value.iloc[0])\n",
    "                        print(f\"  ✓ {col}: Filled {missing_count} values with mode ('{mode_value.iloc[0]}')\")\n",
    "                    else:\n",
    "                        df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "                        print(f\"  ✓ {col}: Filled {missing_count} values with 'Unknown'\")\n",
    "                \n",
    "                # Date columns\n",
    "                elif 'date' in col.lower():\n",
    "                    # For dates, we might want to use forward fill or interpolation\n",
    "                    df_clean[col] = df_clean[col].fillna(method='ffill')\n",
    "                    print(f\"  ✓ {col}: Forward filled {missing_count} date values\")\n",
    "        \n",
    "        cleaned_datasets[name] = df_clean\n",
    "        print(f\"   {name} dataset cleaned successfully\")\n",
    "    \n",
    "    return cleaned_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f85c6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_quality(datasets):\n",
    "    \"\"\"Validate data after cleaning and create business rules\"\"\"\n",
    "    print(\"\\n Data Validation & Business Rules\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        print(f\"\\n {name.upper()} Dataset Validation:\")\n",
    "        issues = []\n",
    "        \n",
    "        # Check for remaining missing values\n",
    "        remaining_missing = df.isnull().sum().sum()\n",
    "        if remaining_missing > 0:\n",
    "            issues.append(f\"Still has {remaining_missing} missing values\")\n",
    "        else:\n",
    "            print(\"   No missing values remaining\")\n",
    "        \n",
    "        # Business rule validations\n",
    "        if 'credit_score' in df.columns:\n",
    "            invalid_credit = df[(df['credit_score'] < 300) | (df['credit_score'] > 850)]\n",
    "            if len(invalid_credit) > 0:\n",
    "                issues.append(f\"Found {len(invalid_credit)} invalid credit scores\")\n",
    "            else:\n",
    "                print(\"   All credit scores are valid (300-850)\")\n",
    "        \n",
    "        if 'yearly_income' in df.columns:\n",
    "            negative_income = df[df['yearly_income'] < 0]\n",
    "            if len(negative_income) > 0:\n",
    "                issues.append(f\"Found {len(negative_income)} negative income values\")\n",
    "            else:\n",
    "                print(\"   All income values are positive\")\n",
    "        \n",
    "        if 'amount' in df.columns:\n",
    "            # Check for extreme outliers (amounts > $10,000 might need investigation)\n",
    "            extreme_amounts = df[abs(df['amount']) > 10000]\n",
    "            if len(extreme_amounts) > 0:\n",
    "                print(f\"    Found {len(extreme_amounts)} transactions > $10,000 (flagged for review)\")\n",
    "        \n",
    "        if 'current_age' in df.columns:\n",
    "            invalid_age = df[(df['current_age'] < 18) | (df['current_age'] > 120)]\n",
    "            if len(invalid_age) > 0:\n",
    "                issues.append(f\"Found {len(invalid_age)} invalid ages\")\n",
    "            else:\n",
    "                print(\"   All ages are realistic (18-120)\")\n",
    "        \n",
    "        validation_results[name] = issues\n",
    "        \n",
    "        if len(issues) == 0:\n",
    "            print(\"   Dataset passed all validation checks!\")\n",
    "        else:\n",
    "            print(\"    Issues found:\")\n",
    "            for issue in issues:\n",
    "                print(f\"    - {issue}\")\n",
    "    \n",
    "    return validation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0fe9d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_summary(datasets):\n",
    "    \"\"\"Create comprehensive data summary for dashboard planning\"\"\"\n",
    "    print(\"\\n Data Summary for Dashboard Planning\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    summary = {}\n",
    "    \n",
    "    # Master dataset key metrics\n",
    "    if 'master' in datasets:\n",
    "        master_df = datasets['master']\n",
    "        \n",
    "        print(\"\\n KEY METRICS FOR DASHBOARD:\")\n",
    "        \n",
    "        # Customer metrics\n",
    "        unique_customers = master_df['client_id'].nunique()\n",
    "        print(f\"   Unique Customers: {unique_customers:,}\")\n",
    "        \n",
    "        # Transaction metrics\n",
    "        total_transaction_volume = master_df['amount_abs'].sum()\n",
    "        avg_transaction = master_df['amount_abs'].mean()\n",
    "        print(f\"   Total Transaction Volume: ${total_transaction_volume:,.2f}\")\n",
    "        print(f\"   Average Transaction: ${avg_transaction:.2f}\")\n",
    "        \n",
    "        # Risk metrics\n",
    "        if 'dark_web_binary' in master_df.columns:\n",
    "            dark_web_cards = master_df['dark_web_binary'].sum()\n",
    "            print(f\"   Cards on Dark Web: {dark_web_cards}\")\n",
    "        \n",
    "        # Geographic spread\n",
    "        unique_states = master_df['merchant_state'].nunique()\n",
    "        unique_cities = master_df['merchant_city'].nunique()\n",
    "        print(f\"    Geographic Reach: {unique_states} states, {unique_cities} cities\")\n",
    "        \n",
    "        # Time range\n",
    "        if 'date' in master_df.columns:\n",
    "            date_range = f\"{master_df['date'].min().strftime('%Y-%m-%d')} to {master_df['date'].max().strftime('%Y-%m-%d')}\"\n",
    "            print(f\"   Transaction Period: {date_range}\")\n",
    "        \n",
    "        # Card brand distribution\n",
    "        if 'card_brand' in master_df.columns:\n",
    "            print(f\"\\n Card Brand Distribution:\")\n",
    "            brand_dist = master_df['card_brand'].value_counts()\n",
    "            for brand, count in brand_dist.head().items():\n",
    "                print(f\"    {brand}: {count:,} ({count/len(master_df)*100:.1f}%)\")\n",
    "        \n",
    "        summary['master_metrics'] = {\n",
    "            'unique_customers': unique_customers,\n",
    "            'total_volume': total_transaction_volume,\n",
    "            'avg_transaction': avg_transaction,\n",
    "            'geographic_states': unique_states,\n",
    "            'geographic_cities': unique_cities\n",
    "        }\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "076d529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the clean data \n",
    "def save_clean_datasets(datasets, folder_path=\"cleaned_data\"):\n",
    "    \"\"\"Save cleaned datasets to CSV files\"\"\"\n",
    "    import os\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        file_path = os.path.join(folder_path, f\"{name}_clean.csv\")\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\" Saved {name} dataset to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50f9536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Complete Data Preparation...\n",
      " Missing Values Analysis\n",
      "==================================================\n",
      "\n",
      " USERS Dataset Missing Values:\n",
      "             Missing_Count  Missing_Percentage\n",
      "age_segment              1                0.05\n",
      "\n",
      " Critical Missing Columns (>10%):\n",
      " No critical missing values found\n",
      "\n",
      " CARDS Dataset Missing Values:\n",
      " No missing values\n",
      "\n",
      " TRANSACTIONS Dataset Missing Values:\n",
      "                Missing_Count  Missing_Percentage\n",
      "errors                  98457               98.46\n",
      "zip                     12426               12.43\n",
      "merchant_state          11741               11.74\n",
      "\n",
      " Critical Missing Columns (>10%):\n",
      "                Missing_Count  Missing_Percentage\n",
      "errors                  98457               98.46\n",
      "zip                     12426               12.43\n",
      "merchant_state          11741               11.74\n",
      "\n",
      " MASTER Dataset Missing Values:\n",
      "                Missing_Count  Missing_Percentage\n",
      "errors                  98457               98.46\n",
      "zip                     12426               12.43\n",
      "merchant_state          11741               11.74\n",
      "age_segment               116                0.12\n",
      "\n",
      " Critical Missing Columns (>10%):\n",
      "                Missing_Count  Missing_Percentage\n",
      "errors                  98457               98.46\n",
      "zip                     12426               12.43\n",
      "merchant_state          11741               11.74\n",
      "\n",
      " Handling Missing Values...\n",
      "==================================================\n",
      "\n",
      " Processing USERS dataset...\n",
      "   users dataset cleaned successfully\n",
      "\n",
      " Processing CARDS dataset...\n",
      "   cards dataset cleaned successfully\n",
      "\n",
      " Processing TRANSACTIONS dataset...\n",
      "  ✓ merchant_state: Filled 11741 values with 'Unknown'\n",
      "  ✓ zip: Filled 12426 values with 'Unknown'\n",
      "  ✓ errors: Filled 98457 values with 'No Error'\n",
      "   transactions dataset cleaned successfully\n",
      "\n",
      " Processing MASTER dataset...\n",
      "  ✓ merchant_state: Filled 11741 values with 'Unknown'\n",
      "  ✓ zip: Filled 12426 values with 'Unknown'\n",
      "  ✓ errors: Filled 98457 values with 'No Error'\n",
      "   master dataset cleaned successfully\n",
      "\n",
      " Data Validation & Business Rules\n",
      "==================================================\n",
      "\n",
      " USERS Dataset Validation:\n",
      "   All credit scores are valid (300-850)\n",
      "   All income values are positive\n",
      "   All ages are realistic (18-120)\n",
      "    Issues found:\n",
      "    - Still has 1 missing values\n",
      "\n",
      " CARDS Dataset Validation:\n",
      "   No missing values remaining\n",
      "   Dataset passed all validation checks!\n",
      "\n",
      " TRANSACTIONS Dataset Validation:\n",
      "   No missing values remaining\n",
      "   Dataset passed all validation checks!\n",
      "\n",
      " MASTER Dataset Validation:\n",
      "   All credit scores are valid (300-850)\n",
      "   All income values are positive\n",
      "   All ages are realistic (18-120)\n",
      "    Issues found:\n",
      "    - Still has 116 missing values\n",
      "\n",
      " Data Summary for Dashboard Planning\n",
      "==================================================\n",
      "\n",
      " KEY METRICS FOR DASHBOARD:\n",
      "   Unique Customers: 1,219\n",
      "   Total Transaction Volume: $5,310,799.01\n",
      "   Average Transaction: $53.11\n",
      "   Cards on Dark Web: 0\n",
      "    Geographic Reach: 118 states, 4819 cities\n",
      "   Transaction Period: 2010-01-01 to 2019-10-31\n",
      "\n",
      " Card Brand Distribution:\n",
      "    Mastercard: 53,694 (53.7%)\n",
      "    Visa: 37,483 (37.5%)\n",
      "    Amex: 6,294 (6.3%)\n",
      "    Discover: 2,529 (2.5%)\n",
      " Saved users dataset to cleaned_data\\users_clean.csv\n",
      " Saved cards dataset to cleaned_data\\cards_clean.csv\n",
      " Saved transactions dataset to cleaned_data\\transactions_clean.csv\n",
      " Saved master dataset to cleaned_data\\master_clean.csv\n",
      "\n",
      " Data Preparation Complete!\n",
      " Ready for Dashboard Development!\n"
     ]
    }
   ],
   "source": [
    "# Main function to run complete missing value treatment\n",
    "def complete_data_preparation(datasets):\n",
    "    \"\"\"Complete data preparation pipeline\"\"\"\n",
    "    print(\" Starting Complete Data Preparation...\")\n",
    "    \n",
    "    # Step 1: Analyze missing values\n",
    "    missing_analysis = analyze_missing_values(datasets)\n",
    "    \n",
    "    # Step 2: Handle missing values\n",
    "    cleaned_datasets = handle_missing_values(datasets)\n",
    "    \n",
    "    # Step 3: Validate data quality\n",
    "    validation_results = validate_data_quality(cleaned_datasets)\n",
    "    \n",
    "    # Step 4: Create summary for dashboard planning\n",
    "    summary = create_data_summary(cleaned_datasets)\n",
    "\n",
    "    # Step 5: Save the clean data\n",
    "    save_clean_datasets(cleaned_data, folder_path=\"cleaned_data\")\n",
    "    \n",
    "    print(\"\\n Data Preparation Complete!\")\n",
    "    print(\" Ready for Dashboard Development!\")\n",
    "    \n",
    "    return cleaned_datasets, validation_results, summary\n",
    "\n",
    "# #####\n",
    "cleaned_data, validation, summary = complete_data_preparation(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
